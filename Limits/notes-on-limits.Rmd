Modeling with Limits and Series
========================================================

Danny Kaplan

```{r echo=FALSE,results="hide",warning=FALSE,message=FALSE}
require(mosaic)
```

Judging from college catalog descriptions of calculus courses, limits and series are a standard part of the first-year college calculus curriculum.  In informal surveys of mathematics instructors, they are identified as central components of calculus, along with derivatives and integrals.

In teaching calculus in a modeling-based way, it's worth thinking about why limits and series are included.  Of course there are benefits to teaching about limits and series --- more knowledge is better! --- but there are also costs.  In particular, the emphasis on limits and infinite series can divert students 
from important modeling concepts such a "big" and "small" and "close."

The mathematical approach to "big" and "small" corresponds to $\lim_{x\rightarrow \infty}$ and $\lim_{x\rightarrow 0}$, but in modeling it's almost always necessary to talk about big and small *compared* to something else.  The mathematical take on series is about convergence, whereas in modeling it's about approximation.  Insofar as calculus is taught as an approach to exactitude and equality, it may interfere with the development of an understanding of approximation, "good enough", and the relevant physical domain of a model.  Traditionally, calculus pushes things to the limit, but in practice you may need "small, but not too small." 

Despite these pitfalls when modeling is a goal, teaching about limits and infinite series may have advantages.
* As a technique used in modeling and applications  
* As a prelude to teaching other techniques, such as integrals and derivatives


Limits 
------

I'll stipulate that there are no applications of limits that are general enough to make them worthwhile to teach as a modeling technique.  One way to back up such a statement is to ask non-mathematics colleagues how they use limits in their work.  The typical answer will be, "I don't."  But it's hard to prove a negative of this sort, so I invite dissent and examples of limits in modeling.  

It's risky to assume that teaching about limits in the traditional way will somehow translate into a better student understanding of modeling; as mentioned above, there are important ways in which the mathematical concept of limits conflicts with modeling. Nonetheless, even if limits aren't used directly in modeling, they may support understanding concepts and techniques that are important to modeling.  

In strategizing about how and why to teach limits in calculus, it is helpful to distinguish between two sorts of limits: limits in the middle and limits at infinity

## Limits in the middle


Calculus textbooks often show functions with point holes in the domain, as in the picture:
```{r title="f-with-hole",echo=FALSE,fig.height=2,fig.width=3}
f <- rfun(~x, seed=8293)
plotFun(f(x)~x, x.lim=c(-3,3),lwd=2)
pts = data.frame( y=f(1),x=1)
plotPoints( y~x, data=pts, pch=20, col="white",add=TRUE)
plotPoints( y~x, data=pts, col="blue",add=TRUE)
``` 

The main instance of this is a derivative:
$$ \frac{df}{dx} \approx \frac{f(x+h)-f(x)}{h} .$$
It's rare to find modeling functions that have a hole in the domain, and $df/dx$ does not generally have a hole when considered as a function of $x$.  It's as a function of $h$ that $df/dx$ has a hole: the division by $h$ means that when $h=0$ the function is undefined by virtue of division
by zero.

To illustrate, consider the derivative of $A \cos(\frac{2\pi}{p} t)$ at $t=1$ with some specific values of the parameters $A$ and $p$, say, $A=2$ and $p=5$:
```{r title="deriv-vs-h",echo=TRUE,fig.height=2,fig.width=3}
f = makeFun(A*sin(2*pi*t/p)~t, A=2,p=5)
dfdx = makeFun((f(t+h)-f(t))/h ~ t)
plotFun( dfdx(t=1,h) ~ h,h.lim=range(-0.1,0.1),lwd=2)
```

Where's the hole in the graph? We know the function $df/dx$ is undefined when $h=0$, but the graph doesn't show it.

It's not that the plotting software knows about limits and has done the appropriate calculation of the limit.  The computer recognizes that the ratio being plotted is undefined for $h=0$ and will return an indication of this.
```{r}
dfdx(t=1,h=0)
```
The plotting software is arranged so that it ignores points at which ```NaN``` is the value. Drawing a simple linear interpolation between the points at which the function is defined gives a sensible read-out of the function.  Picking any small value of $h$ will give a very close approximation to the derivative:
```{r}
fprime = D(f(t)~t)
fprime(t=1)
dfdx(t=1,h=.0001)
dfdx(t=1,h=.000001)
dfdx(t=1,h=.00000001)
```

Since we can't define the derivative using $h=0$, we need to choose some other value of $h$. There are aesthetic reasons for choosing to define the derivative as a limit $h \rightarrow 0$ --- you would be rightly repulsed at the idea of defining a basic concept like the derivative with a specific, non-zero $h$, for instance, $h=0.000001$.

But the practical reason to define the derivative in terms of the limit $h \rightarrow 0$ is to be able to leave $h$ out of the conversation entirely.  By using the limit, we get the advantages of making $h$ small without the need to define what "small"" means.

From a modeling perspective, the meaning of "small" is important and we should engage it directly when teaching about derivatives.  Why?

  * Often the purpose for which a derivative is used is to indicate the rate of change of a quantity --- how the output changes as the input is changed.  In practice, the input will always be changed by a finite amount and so the output will change accordingly.  Using the derivative defined in terms of the limit will give only an approximation to the change over a necessarily finite change in the input. 
  * Measurements of derivatives are often made from discrete measurements.  How fast does the grass grow?  The sensible thing is to measure the length one day, then come back the next day.  Would you ever want to measure the length one instant and then measure it again after one second or one millisecond, etc?  Each measurement invariably has error.  The derivative you measure is really
$$\widehat{df/dt} = f(t+h)+\epsilon_1 - (f(t)+\epsilon_2)/h$$
where $\epsilon_1$ and $\epsilon_2$ are random errors.  Re-arranging shows the problem:
$$\widehat{df/dt} = \frac{f(t+h)-f(t)}{h} + \frac{\epsilon_1-\epsilon_2}{h}.$$

In practice, $\epsilon_1 - \epsilon_2 \neq 0$.  Making $h$ big helps to reduce the influence of measurement error on the result, even if it introduces a systematic bias into the value of $\frac{f(t+h)-f(t)}{h}$ compared to the theoretical derivative based on the limit.

In the context of computer arithmetic, the errors of rounding in floating point calculations introduce non-zero $\epsilon_1$ and $\epsilon_2$.  The result is that numerical calculations of finite-difference derivatives are done with an $h$ of moderate size: proportional to $\sqrt{|\epsilon_1|}$.  

In the context of measurements made in the field or lab, there are additional sources of error.  These argue for a finite $h$.  Why emphasize the limit to the exclusion of teaching some sensible about defining "small".

<blockquote>
  * Body temperature varies slowly and systematically by about $\pm 1$ F over the course of the day: a so-called "circadian rhythm."  Suppose you wanted to measure the rate of change of body temperature at 6am.  What factors would influence your choice of $h$?  

ANS: Precision of the thermometer.  Typically, you can measure body temperature to a precision of about 0.1 F.  So in choosing $h$ you would want to have a time interval over which the change in body temperature is bigger than 0.1 F.
</blockquote>


Separating the measurements by a good span of time serves to make the influence of the error smaller.

### Limits at Infinity

One of the principles learned by many, perhaps most, students is that "you can't divide by zero."  Yet from time to time one is forced to deal with an expression that involves division by a quantity that might be zero.  In the case of derivatives, the quantity is the step $h$.  The notation $h \rightarrow 0$ reminds the user to make $h$ small but to make sure $h\neq 0$.  

The algebraic techniques undercut this message of "small but not zero."  The basic algebraic process is to manipulate the expression symbolically, treating $h$ as if it were not zero (to avoid violating the can't-divide-by-zero principle).  Then, when one finally arrives at an expression that doesn't involve any division by $h$, feel free to plug in zero.

With respect to derivatives, the student will soon move on to the symbolic transformations that implement differentiation and can forget all about limits.  The facts that will remain are rules like $x^2 \rightarrow 2 x$ and "a derivative is a slope or tangent" (sic).  

But there are times when division by zero occurs.  For instance, in September 1997, a crew member on a US Navy missile cruiser, the USS Yorkdown, entered a zero into a database field.[ref](http://en.wikipedia.org/wiki/USS_Yorktown_(CG-48))  Subsequently, this value was used as the denominator in a division calculation.  The divide by zero produced a machine error which cascaded into the failure of a computer network on the ship.  This caused the ship's propulsion to fail.  The Yorktown was dead in the water for 2 hours and 45 minutes.

Computer engineers have overcome the objections of third-grade teachers and recognized that division by zero is not really an error but a situation that needs to be acknowledged.  The IEEE floating point standard, the arithmetic standard used on most computers today, treats division by zero as an "exception" not an error; a valid result is returned.
```{r}
1/0
```
You can do arithmetic with ```Inf``` just as if it were a number.
```{r}
1/Inf
3 + Inf
-4*Inf
```

There are indeterminate cases that are treated sensibly by the standard, signalled by a value of ```NaN``` --- "not a number".
```{r}
0*Inf
Inf - Inf
Inf/Inf
0/0
```

Since such arithmetic is an element of the technical life of our students, something they can and will invoke often in their quantitative studies, perhaps a good application for limits is to explain why it makes sense to set up the rules this way.  


There are other times when multiplication by infinity needs to be considered.  For instance, one form of "surge" function is $f(t) = A t \exp(-kt)$.  In the long run, as $t$ becomes large, what will happen to this function?  It is a product of two functions: $t$ increases steadily while $\exp(-kt)$ goes to zero as $t$ increases. The simple arithmetic rules are indeterminate:
```{r}
t = Inf
t*exp(-t)
```

A plot resolves the problem:
```{r label="surge-plot",fig.height=2,fig.width=3}
plotFun( A*t*exp(-k*t)~t, t.lim=c(0,100),k=.1,A=2)
```

The function goes to zero on a time scale that's determined by $k$, not $A$.  Perhaps even introduce an interactive script to allow students to vary the parameters and see which ones are important.
```{r eval=FALSE}
require(manipulate)
manipulate(
  plotFun(A*t*exp(-k*t)~t,t.lim=c(0,10^n),k=k,A=A,npts=10000),
  n = slider(0,5,step=.1,initial=2,label="Set t-max"),
  k = slider(0,.2,step=.001,initial=.1),
  A = slider(-100,100,step=1,initial=3)
  )
```

The traditional way to show that the surge function goes to zero as $t \rightarrow \infty$ is by l'Hospital's rule.  First published in 1696, the Rule seems mainly to provide a mechanism, a pseudo-application, for having students take derivatives of a variety of functions and spotting singularities, places where the input to a function would cause a violation of the no-division-by-zero rule.  Whether this is a help to students or a distraction is a matter of dispute, but the textbook cases are appropriately handled by the rules for computer arithmetic and the conventions of the plotting program.

#### Exercises: Functions at zero and infinity
  * $\displaystyle \lim_{x \to 1} \frac{\ln x}{x-1}$
  * $\displaystyle \lim_{x \to 1} \frac{x^2-1}{2*x^2+1}$
  * $\displaystyle \lim_{x \to \infty} \frac{e^x}{x^2}$
  * $\displaystyle \lim_{x \to \infty} \frac{\ln x}{\sqrt[3]{x}}$
  * $\displaystyle \lim_{x \to 0^+} \frac{x}{1/\ln x}$
  * A counter-example from Stewart where l'Hospital's rule shouldn't be applied: $latex \displaystyle \lim_{x \to \pi^-} \frac{\sin x}{1 - \cos x}$

Show that in each of these cases, the computer arithmetic results in ```NaN``` at the singularity, but the plot shows the limit reasonably.

#### Summary: Teaching limits in a modeling-based way:
  * When introducing the derivative, $h \rightarrow 0$ means that $h$ should be small.  Discuss what "small" means and how it depends on context.  Two important contexts: the length scale over which the function shows visible curvature; the amount of measurement (or round-off) noise in an evaluation of the function.
  * The idea that division by zero is prohibited and how the computer deals with a division by zero.  Quantities that make sense near zero (or near infinity) are reasonably interpreted in the limit.


Series
------

Sequences and series are often motivated by Zeno's paradoxes.  
The problem with this motivation is that, when the interest is in describing the world, the paradoxes not very motivating.  Everyone knows from childhood that the quicker runner will overtake the slower and that the arrow will reach its target.
Similarly, everyone knows that a circle has an area and that it's sensible to speak of the slope of a curve or the area under a curve. Proving such things is not on the modeler's agenda.

Reserve Zeno's paradoxes for demonstrating that there are situations where simple logic that sounds compelling is wrong and where a formal and precise approach is needed to avoid error.  Zeno provides a cautionary lesson in critical thinking, but calculus has more important uses than demonstrating errors in informal logic.  If you want an example where wrong thinking leads to trouble, you need not appeal to the arrows and races of antiquity but to current reasoning about money and debt and the tempting logic that a country's finances are similar to a family's. 

Much of modeling concerns approximation.  The relationship to series does not involve infinite series but *finite* series.  The concern is not whether the infinite series will converge but whether the finite series is close enough.

The modeling goals of teaching series might be considered to be:
  1. You can usefully break things into components, and it can happen that considering only a few of the components is enough to get the job done.
  2. Mastering the most common general techniques for dividing things into parts, e.g. polynomial approximation.
  3. In refining the understanding of (2), knowing that there are situations where taking just a few parts doesn't accomplish what's wanted. 

To illustrate the difference between convergence of series and modeling, consider a gambler heading off to a casino to play roulette.  The gambler has decided on a simple play --- always bet on red --- and is seeking a strategy that will guarantee this. Reading the literature, the gambler is tempted by the doubling-down strategy:
<blockquote>
When you win, walk away a winner.  Until then, when you lose, double your bet in the next round.
</blockquote>

A single bet on red will win 10 out of 21 times, giving an expectation value of $-0.053$ for every dollar bet.  But the doubling-down strategy is a sure win.  If you lose \$1 in the first round, your bet of \$2 in the second will yield a net win of $1 if you win.  If you happen to lose the second round, your doubled bet of \$4 in the third round will yield a net win of \$1 --- the 4 dollars you win in that round minus the 3 you lost in the two previous rounds.  And so on, ad infinitum.  Eventually the ball must land on red. When it does, you will clear \$1.  A sure win!  Indeed, if rather than betting on red, the gambler always bet on number 7, the sure win would be \$35.

The above paragraph is an analysis of a model of the gambling session.  Like all models, it leaves some things out.  Sensibly, the model neglects the possibility that the casino will be hit by an asteroid, wiping out your potential win.  Not so sensibly, the model assumes that you can always double-down, which requires your having access to an infinite amount of money.  It also assumes that there is no cap by the casino on the amount of a bet, that the casino is willing to accept a bet of an arbitrary amount of money.

The process can't go out to infinity because either you will run out of money or the casino will step in.  The probability of losing is small.  How small? It depends on the amount of money you have and the house limit on bets.  But it is not zero.  And if you lose, you may lose big.  So the guaranteed win is really nothing of the kind.  The idealization of an infinite process misses the essential feature that causes the strategy to fail.

#### Where Series Fit in Calculus

The best-selling calculus textbook is Stewart.  Here's the table of contents for the 2nd edition, often used as the single text for a three-semester sequence of calculus.
  1. Functions and Models
  2. Limits and Derivatives
  3. Differentiation Rules
  4. Applications of Differentiation
  5. Integrals
  6. Applications of Integration
  7. Differential Equations 
  8. **Infinite Sequences and Series**
  9. Vectors and the Geometry of Space
  10. Vector Functions
  11. Partial Derivatives
  12. Multiple Integrals
  13. Vector Calculus

Note that "Infinite Sequences and Series" comes after differential and integral calculus, suggesting that the material is not needed to introduce differentiation and integration.  Anecdotally, instructors often report not being able to reach sequences and series in the first year of calculus.  More than three quarters of students who study calculus never reach past the first year, suggesting that the point of sequences and series is not to illuminate integration and differentiation.

Are there applications of sequences and series that are important? Economists rightly point to the concept of "net present value," the equivalent in today's money of a stream of income and expenditure in the future.  A mathematically similar problem is that of the distance travelled by a bouncing ball: a ball that bounces each time to a fixed fraction of its peak height on the last bounce.  Both of these examples relate to exponentials and sums of exponentials.

#### Net Present Value

In March 2012, a group of investors, including basketball star Magic Johnson, bought the Los Angeles Dodgers at auction for $2 billion. A news commentator, trying to give an idea of how much money this is, calculated that this is enough to buy season tickets for 308,000 years. 

It's easy to see where this estimate comes from.  The Dodger's web site lists the most expensive season ticket at \$80 per game; the least expensive is \$5 per game.  Given that there are 81 home games in a season, the season ticket cost \$6480 in the expensive seats and \$405 in the cheap seats.  So with two billion dollars, one could buy 308,642 expensive tickets or 4,938,272 cheap season tickets.

But this is the wrong calculation.  It ignores several important factors:
* The interest that the money can earn.
* Inflation
As of this writing, 30-year US treasury bonds are earning approximately 3% per year.  Inflation runs at about 2%.  Taken together, a dollar this year will buy goods worth about 1.01 dollars next year, and so on cumulatively.  Put another way, buying a season ticket in $n$ years will cost roughly $1.01^{-n}6480$ dollars in today's money.  To put aside enough money to fund the purchase of a season ticket each year for 300,000 years will involve about \$648,000. $$\sum_{n=1}^{300000} 6480 \frac{1}{1.01}^n \approx 648000 .$$
A substantial sum, but not close to \$2 billion.  Actually, the \$648000 will buy season tickets not just for 300,000 years, but for an infinite time in the future. Season tickets far in the future are very cheap in today's money.  Based on the interest and inflation rates assumed here, to buy an infinite sequence of season tickets starting just 2000 years in the future costs less than a quarter of a cent in today's money.

The important mathematics here is not that the sequence of present values of future season tickets converges.  What's important is the time scale on which the present value becomes trivial.  At 1\% discount per year, the half-life is about 72 years.  Ten half lives corresponds to a decrease by a factor of 1000.  So an estimate that's good to 0.1\% need not look forward more than 1000 years.

Indeed, there's little point in worrying about what it's going to cost after even 100 years.  The assumptions about interest yields and inflation rates, to say nothing of the very existence of a baseball season or the Dodgers themselves, introduce a much larger error.

The exponential process guarantees convergence.  It's the time scale of the process that needs emphasis.

Let's turn the problem around and imaging that Magic Johnson and his co-investors decided to use the two-billion dollars fund college tuition for one person each year until the money runs out.  College tuition has, for the past 60 years, been rising at an average rate of about 6% per year.  Currently, for the most expensive colleges, it's rising at about 4.5% per year.  Given the 3% return on long-term treasury bonds, paying for a year of college far in the future will cost *more* in today's money than paying for a sooner year.  Using \$50,000 per year as the cost of an elite college, the money will run out when
$$ 50,000 \times \sum_{n=1}^{k} 1.015^k = 2,000,000,000.$$
Solving for $k$ involves looking at finite sums, not the convergence of the series.  Trial and error works reasonably well:
```{r}
50000*sum(1.015^(1:500)) # more than 2 billion
50000*sum(1.015^(1:250)) # less than 2 billion
50000*sum(1.015^(1:400)) # less than 2 billion
50000*sum(1.015^(1:450)) # more than 2 billion
50000*sum(1.015^(1:425)) # Close!
```

So, the two-billion dollars will pay for one student to attend college each year for the next roughly 425 years.  There's no point in worrying about the solution more precisely, since the inflation and interest rates influence the calculation sensitively.  For instance, if the two-billion dollars can be invested in a way that earns 4% yearly rather than the assumed 3%, the money will pay for 1060 years of college.  On the other hand, if tuition costs rise at 5% in face of 3% investment returns, the two-billion dollars will cover only 185 years of college.  

Of course, nobody who understands exponential growth believes that the college tuition system can continue this way.  Even 20 years is beyond the tuition and investment planning horizon of administrators in higher education.

#### Approximation with Polynomials

Another topic encountered is Taylor polynomials and other, related approximations.[^1]

Representation of arbitrary functions with polynomials.  Low order ... things work well.[^3]  High order ... pathologies:

[^1]: 
  testing a footnote

[^3]:
  Another footnote.
  
And a table:

   Item      | Value 
   --------- | ------
   Computer  | $1600 
   Phone     |   $12 
   Pipe      |    $1 

  * quadratic or higher runs off to $\pm \infty$ very quickly.  Not useful for extrapolation.
  * Estimation becomes hard with high-order polynomials, due to non-orthogonality.  
  * Selection of model order: use anova, so that you look at the incremental improvement of adding a new term
  * Parameter estimation: variance inflation due to collinearity.

The general lessons of experience in science is: 
  * Use first or at most second order.  
  * Use multiple variables rather than high-order in a single variable.  
  * Try to make the multiple variables orthogonal (by randomization or orthogonal assignment) to avoid variance inflation.  
The important concept is orthogonality, not convergence.




`` You stole my net present value example."



